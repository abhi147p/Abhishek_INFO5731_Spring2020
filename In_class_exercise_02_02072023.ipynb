{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhi147p/Abhishek_INFO5731_Spring2023/blob/main/In_class_exercise_02_02072023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78eoaRW4IzZL"
      },
      "source": [
        "## The second In-class-exercise (02/07/2023, 40 points in total)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAk3p4NrIzZM"
      },
      "source": [
        "The purpose of this exercise is to understand users' information needs, then collect data from different sources for analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrvHCQeQIzZN"
      },
      "source": [
        "Question 1 (10 points): Describe an interesting research question (or practical question) you have in mind, what kind of data should be collected to answer the question(s)? How many data needed for the analysis? The detail steps for collecting and save the data. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ig56YCOIzZO"
      },
      "outputs": [],
      "source": [
        "# Your answer here (no code for this question, write down your answer as detail as possible for the above questions):\n",
        "\n",
        "'''\n",
        "Please write you answer here:\n",
        "\n",
        "Now a days, Machine Learning is rapidly rising towards future everywhere, where it needs huge amount of data for performing different algorithms by analysing\n",
        "it. We generally use different process to collect and analyse data using ML algorithms for different topics such as regression analysis to categorise\n",
        "the data into different parts and clustering the data etc. What if we want to know:\n",
        "-> what all topics are trending over machine learning\n",
        "-> what new packages have been deployed and packages that are deprecated.\n",
        "\n",
        "Here the data, I want, will get through reddit website.\n",
        "\n",
        "Research Questions:\n",
        "\n",
        "-> What process is needed to acheive such thing?\n",
        "-> How can we find such solution based on data? \n",
        "-> What algorithms models are needed for our analysis?\n",
        "-> How does sentiment analysis works over our data, that we scrap through website.\n",
        "\n",
        "The data that will be needed in this process is, user_id, title, description, number_of_upvotes and downvotes, so that we can get to know which all \n",
        "posts are on trending. For analysis of this process, we need a huge amount of data, around 3000-5000 records of data\n",
        "\n",
        "The steps to collect data:\n",
        "-> For this process, we use reddit site to scrape.\n",
        "-> With the help of praw api, we will scrape the reddit website data and save it in csv format.\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERy3NkcJIzZO"
      },
      "source": [
        "Question 2 (10 points): Write python code to collect 1000 data samples you discussed above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "0sTtTui-IzZO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "000891ec-52f5-453a-89f6-4f0779b90342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "user_name: Abhishek_Abhi147\n",
            "Enter number of posts: 1000\n",
            "The size of data: 998\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       Id                                              Title Description  \\\n",
              "0  gh1dj9  [Project] From books to presentations in 10s w...               \n",
              "1  kuc6tz  [D] A Demo from 1993 of 32-year-old Yann LeCun...               \n",
              "2  g7nfvb  [R] First Order Motion Model applied to animat...               \n",
              "3  lui92h  [N] AI can turn old photos into moving Images ...               \n",
              "4  ohxnts  [D] This AI reveals how much time politicians ...               \n",
              "\n",
              "           created_at  Likes or upvotes  \n",
              "0 2020-05-10 13:19:54              7731  \n",
              "1 2021-01-10 10:30:36              5799  \n",
              "2 2020-04-25 04:27:23              4742  \n",
              "3 2021-02-28 15:12:28              4665  \n",
              "4 2021-07-11 04:18:59              4520  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bad9e96b-7f87-4129-98ab-ffe56c210478\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Id</th>\n",
              "      <th>Title</th>\n",
              "      <th>Description</th>\n",
              "      <th>created_at</th>\n",
              "      <th>Likes or upvotes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>gh1dj9</td>\n",
              "      <td>[Project] From books to presentations in 10s w...</td>\n",
              "      <td></td>\n",
              "      <td>2020-05-10 13:19:54</td>\n",
              "      <td>7731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>kuc6tz</td>\n",
              "      <td>[D] A Demo from 1993 of 32-year-old Yann LeCun...</td>\n",
              "      <td></td>\n",
              "      <td>2021-01-10 10:30:36</td>\n",
              "      <td>5799</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>g7nfvb</td>\n",
              "      <td>[R] First Order Motion Model applied to animat...</td>\n",
              "      <td></td>\n",
              "      <td>2020-04-25 04:27:23</td>\n",
              "      <td>4742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>lui92h</td>\n",
              "      <td>[N] AI can turn old photos into moving Images ...</td>\n",
              "      <td></td>\n",
              "      <td>2021-02-28 15:12:28</td>\n",
              "      <td>4665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ohxnts</td>\n",
              "      <td>[D] This AI reveals how much time politicians ...</td>\n",
              "      <td></td>\n",
              "      <td>2021-07-11 04:18:59</td>\n",
              "      <td>4520</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bad9e96b-7f87-4129-98ab-ffe56c210478')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bad9e96b-7f87-4129-98ab-ffe56c210478 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bad9e96b-7f87-4129-98ab-ffe56c210478');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import praw, time\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "\n",
        "def get_date(created):\n",
        "  \"\"\"\n",
        "    params1: created_at time from praw\n",
        "    return: converted into datetime format\n",
        "  \"\"\"\n",
        "  return dt.datetime.fromtimestamp(created)\n",
        "\n",
        "def authenticate_reddit():\n",
        "  \"\"\"\n",
        "    return: praw api, that has been authenticated\n",
        "  \"\"\"\n",
        "  reddit = praw.Reddit(\n",
        "      client_id=\"yCq-n_3-AtNM-6V1z97-Gg\",\n",
        "      client_secret=\"ET7VSf-Pg-CuqJNQrvv0WhFm7oPtiA\",\n",
        "      password=\"***********\",\n",
        "      user_agent=\"testscript by u/Abhishek_Abhi147\", #here for default we can use testscript\n",
        "      username=\"Abhishek_Abhi147\",\n",
        "      check_for_async=False\n",
        "  )\n",
        "  return reddit\n",
        "\n",
        "def fetch_reddit_data(reddit, count):\n",
        "  \"\"\"\n",
        "    params1: praw api of reddit\n",
        "    params2: count, number of posts\n",
        "    return: dataframe, which contains N number of posts data that has been scraped\n",
        "  \"\"\"\n",
        "  # using MachineLearning as topic and for the trending posts, we used top operator with number of posts as limit\n",
        "  subreddit_posts = reddit.subreddit('MachineLearning').top(limit=count)\n",
        "  data = []\n",
        "  i = 0\n",
        "  for post in subreddit_posts:\n",
        "    data.append([post.id, post.title, post.selftext, post.created, post.score])\n",
        "    i = i + 1\n",
        "    if i%50 == 0:\n",
        "      time.sleep(3)\n",
        "\n",
        "  df = pd.DataFrame(data, columns=[\"Id\", \"Title\", \"Description\", \"created_at\", \"Likes or upvotes\"])\n",
        "  # converting to datetime format\n",
        "  df['created_at'] = df['created_at'].apply(get_date)\n",
        "  return df\n",
        "\n",
        "reddit = authenticate_reddit()\n",
        "print(f\"user_name: {reddit.user.me()}\")\n",
        "posts_count = int(input(\"Enter number of posts: \"))\n",
        "df = fetch_reddit_data(reddit, posts_count)\n",
        "print(f\"The size of data: {df.shape[0]}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmyz5s-wIzZP"
      },
      "source": [
        "Question 3 (10 points): Write python code to collect 1000 articles from Google Scholar (https://scholar.google.com/), Microsoft Academic (https://academic.microsoft.com/home), or CiteSeerX (https://citeseerx.ist.psu.edu/index), or Semantic Scholar (https://www.semanticscholar.org/), or ACM Digital Libraries (https://dl.acm.org/) with the keyword \"information retrieval\". The articles should be published in the last 10 years (2012-2022).\n",
        "\n",
        "The following information of the article needs to be collected:\n",
        "\n",
        "(1) Title\n",
        "\n",
        "(2) Venue/journal/conference being published\n",
        "\n",
        "(3) Year\n",
        "\n",
        "(4) Authors\n",
        "\n",
        "(5) Abstract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8F5JOFcXIzZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "outputId": "0ec97b04-11dd-480c-d940-b643044fd5db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter num of articles: 100\n",
            "***** BEFORE FETCHING *********\n",
            "******* fetched 10 articles *********\n",
            "******* fetched 20 articles *********\n",
            "******* fetched 30 articles *********\n",
            "******* fetched 40 articles *********\n",
            "******* fetched 50 articles *********\n",
            "******* fetched 60 articles *********\n",
            "******* fetched 70 articles *********\n",
            "******* fetched 80 articles *********\n",
            "******* fetched 90 articles *********\n",
            "******* fetched 100 articles *********\n",
            "******* AFTER FETCHING ********\n",
            "Number of records: 100\n",
            "dimensions of articles: (100, 6)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               Title  \\\n",
              "0   Information retrieval as statistical translation   \n",
              "1  A survey of automatic query expansion in infor...   \n",
              "2  A language modeling approach to information re...   \n",
              "3  A study of smoothing methods for language mode...   \n",
              "4  Open language learning for information extraction   \n",
              "\n",
              "                            Author  Year         Published  \\\n",
              "0             A Berger, J Lafferty  2017        dl.acm.org   \n",
              "1            C Carpineto, G Romano  2012        dl.acm.org   \n",
              "2               JM Ponte, WB Croft  2017        dl.acm.org   \n",
              "3               C Zhai, J Lafferty  2017        dl.acm.org   \n",
              "4  M Schmitz, S Soderland, R Bart‚Ä¶  2012  aclanthology.org   \n",
              "\n",
              "                                            Abstract  \\\n",
              "0  ‚Ä¶ There is a large literature on probabilistic...   \n",
              "1  ‚Ä¶ information retrieval systems is largely cau...   \n",
              "2  ‚Ä¶ models, we have developed an approach to ret...   \n",
              "3  ‚Ä¶ to information retrieval are attractive and ...   \n",
              "4  Open Information Extraction (IE) systems extra...   \n",
              "\n",
              "                                        Abstract_UrL  \n",
              "0  https://dl.acm.org/doi/abs/10.1145/3130348.313...  \n",
              "1  https://dl.acm.org/doi/abs/10.1145/2071389.207...  \n",
              "2  https://dl.acm.org/doi/pdf/10.1145/3130348.313...  \n",
              "3  https://dl.acm.org/doi/abs/10.1145/3130348.313...  \n",
              "4              https://aclanthology.org/D12-1048.pdf  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e1eb5ae4-e3eb-4aec-af08-e71ca0493aa4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Title</th>\n",
              "      <th>Author</th>\n",
              "      <th>Year</th>\n",
              "      <th>Published</th>\n",
              "      <th>Abstract</th>\n",
              "      <th>Abstract_UrL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Information retrieval as statistical translation</td>\n",
              "      <td>A Berger, J Lafferty</td>\n",
              "      <td>2017</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>‚Ä¶ There is a large literature on probabilistic...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/3130348.313...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A survey of automatic query expansion in infor...</td>\n",
              "      <td>C Carpineto, G Romano</td>\n",
              "      <td>2012</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>‚Ä¶ information retrieval systems is largely cau...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/2071389.207...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A language modeling approach to information re...</td>\n",
              "      <td>JM Ponte, WB Croft</td>\n",
              "      <td>2017</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>‚Ä¶ models, we have developed an approach to ret...</td>\n",
              "      <td>https://dl.acm.org/doi/pdf/10.1145/3130348.313...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>A study of smoothing methods for language mode...</td>\n",
              "      <td>C Zhai, J Lafferty</td>\n",
              "      <td>2017</td>\n",
              "      <td>dl.acm.org</td>\n",
              "      <td>‚Ä¶ to information retrieval are attractive and ...</td>\n",
              "      <td>https://dl.acm.org/doi/abs/10.1145/3130348.313...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Open language learning for information extraction</td>\n",
              "      <td>M Schmitz, S Soderland, R Bart‚Ä¶</td>\n",
              "      <td>2012</td>\n",
              "      <td>aclanthology.org</td>\n",
              "      <td>Open Information Extraction (IE) systems extra...</td>\n",
              "      <td>https://aclanthology.org/D12-1048.pdf</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e1eb5ae4-e3eb-4aec-af08-e71ca0493aa4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-e1eb5ae4-e3eb-4aec-af08-e71ca0493aa4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-e1eb5ae4-e3eb-4aec-af08-e71ca0493aa4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_soup(url):\n",
        "    \"\"\"\n",
        "        params1: url (contains the url of google scholar page)\n",
        "        return: soup (fetching the url page data and then further converted to html parser)\n",
        "    \"\"\"\n",
        "  # headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36'}\n",
        "    headers = {'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
        "    try:\n",
        "        # requesting for data using requests with url and headers for authentication\n",
        "        data = requests.get(url, headers)\n",
        "        # print(f\"Extracted the data with response as {data.status_code}\")\n",
        "        if data.status_code != 200:\n",
        "            raise Exception(\"Failed to fetch data\")\n",
        "    except Exception as ex:\n",
        "        print(f\"Exception occurred as {data.text} with status_code {data.status_code}\")\n",
        "        return None\n",
        "    soup = BeautifulSoup(data.content)\n",
        "    return soup\n",
        "\n",
        "def get_title(title):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched title of article\n",
        "      return: string format of text title of article\n",
        "    \"\"\"\n",
        "    return str(title.find(\"a\").text)\n",
        "\n",
        "def get_abstract_url(title):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched title of article\n",
        "      return: string format of url for title of article\n",
        "    \"\"\"\n",
        "    return str(title.find(\"a\").get(\"href\"))\n",
        "\n",
        "\n",
        "def get_article_info(article):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched article info\n",
        "      return: tuples containing author, year, and published info \n",
        "    \"\"\"\n",
        "    # using regular expressions, fetched year from article\n",
        "    year = int(re.search(r'\\d+', article.text).group())\n",
        "    # performing some string operations, fetched required results\n",
        "    article = str(article.text).replace(\"\\xa0\", \"\")\n",
        "    article = article.split(\"-\")\n",
        "    published = article[-1].strip()\n",
        "    author = article[0].strip()\n",
        "    return author, year, published\n",
        "\n",
        "def get_tags(soup):\n",
        "    \"\"\"\n",
        "      params1: soup_api with fetched url and parsed data\n",
        "      return: list of article info, such as titles, authors, year, published, abstract\n",
        "    \"\"\"\n",
        "    # fetched titles and authors of article using findAll by mentioning some tags\n",
        "    all_titles = soup.findAll(\"h3\", attrs={\"class\": \"gs_rt\"})\n",
        "    all_authors = soup.findAll(\"div\", attrs={\"class\": \"gs_a\"})\n",
        "    all_abstracts = soup.findAll(\"div\", attrs={\"class\": \"gs_rs\"})\n",
        "\n",
        "    authors, year, published = [], [], []\n",
        "\n",
        "    titles = [get_title(title) for title in all_titles]\n",
        "    abs_url = [get_abstract_url(title) for title in all_titles]\n",
        "    abstract = [get_abstract(abstr) for abstr in all_abstracts]\n",
        " \n",
        "    for author in all_authors:\n",
        "        auth, yr, publs = get_article_info(author)\n",
        "        authors.append(auth)\n",
        "        year.append(yr)\n",
        "        published.append(publs)\n",
        " \n",
        "    return titles, authors, year, published, abstract, abs_url\n",
        " \n",
        "\n",
        "def get_abstract(abstr):\n",
        "    \"\"\"\n",
        "      params1: soup_Api with fetched abstract of article\n",
        "      return: string format of article abstract by fetching its text\n",
        "    \"\"\"\n",
        "    return str(abstr.text).replace(\"\\n\", \"\")\n",
        "\n",
        "\n",
        "def fetch_web_data(records):\n",
        "    \"\"\"\n",
        "      params1(records): number of articles, needs to be fetched\n",
        "      return: dataframe containing total N number of articles.\n",
        "    \"\"\"\n",
        "    year_st, year_end = 2012, 2022\n",
        "    columns_google = [\"Title\", \"Author\", \"Year\", \"Published\", \"Abstract\", \"Abstract_UrL\"]\n",
        "    # fetching for 1000 articles\n",
        "    titles, authors, years, published, abstract, abs_url = [], [], [], [], [], []\n",
        "    final_data = []\n",
        "    print(\"***** BEFORE FETCHING *********\")\n",
        "    # records = 100 # no of articles\n",
        "    for i in range(0, records, 10):\n",
        "        url = f\"https://scholar.google.com/scholar?start={i}&q=information+retrieval&hl=en&as_sdt=0,44&as_ylo={year_st}&as_yhi={year_end}&as_vis=1\"\n",
        "        soup = get_soup(url)\n",
        "        if soup is None:\n",
        "          print(f\"Data Not Fetched.... for {i} article page\")\n",
        "          continue\n",
        "        # titles, authors, year, published, abstract, abs_url\n",
        "        a, b, c, d, e, f = get_tags(soup)\n",
        "        titles.extend(a)\n",
        "        authors.extend(b)\n",
        "        years.extend(c)\n",
        "        published.extend(d)\n",
        "        abstract.extend(e)\n",
        "        abs_url.extend(f)\n",
        "        print(f\"******* fetched {(i+10)} articles *********\")\n",
        "        # keeping time to sleep for 5 seconds, so that, server may not crash for frequent multiple requests.\n",
        "        time.sleep(5)\n",
        "   \n",
        "    for i in range(records):\n",
        "        final_data.append([titles[i],authors[i],years[i],published[i],abstract[i],abs_url[i]])\n",
        "\n",
        "    print(\"******* AFTER FETCHING ********\")\n",
        "    df = pd.DataFrame(final_data, columns = columns_google)\n",
        "    print(f\"Number of records: {df.shape[0]}\")\n",
        "    return df\n",
        "\n",
        "# url = \"https://scholar.google.com/scholar?start=200&q=information+retrieval&hl=en&as_sdt=0,44&as_ylo=2012&as_yhi=2022&as_vis=1\"\n",
        "# soup = get_soup(url)\n",
        "# all_titles = soup.findAll(\"h3\", attrs={\"class\": \"gs_rt\"})\n",
        "# all_authors = soup.findAll(\"div\", attrs={\"class\": \"gs_a\"})\n",
        "# all_abstracts = soup.findAll(\"div\", attrs={\"class\": \"gs_rs\"})\n",
        "\n",
        "df = fetch_web_data(int(input(\"Enter num of articles: \")))\n",
        "print(f\"dimensions of articles: {df.shape}\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-wwSIh_IzZP"
      },
      "source": [
        "Question 4 (10 points): Write python code to collect 1000 posts from Twitter, or Facebook, or Instagram. You can either use hashtags, keywords, user_name, user_id, or other information to collect the data. \n",
        "\n",
        "The following information needs to be collected:\n",
        "\n",
        "(1) User_name\n",
        "\n",
        "(2) Posted time\n",
        "\n",
        "(3) Text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "LbRQlenHIzZP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "d50e9453-be1a-4a45-c7e9-d9a2b9cd5f58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of tweets want to fetch: 1000\n",
            "Enter the hashtag: Sports\n",
            "dimensions of tweets: (1000, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          username                                               text  \\\n",
              "0       MGuindonRC  Belleterre est le dernier endroit au Qu√©bec qu...   \n",
              "1  TesoroMoctezuma  üòÜüßãü•™#FelizDomingo!#EnPortada #Portada de\\n#Port...   \n",
              "2        verde2730  RT @SportsHochi: #T„É™„Éº„Ç∞ „ÉªÊù±‰∫¨„Åå„Éï„Ç°„Ç§„Éä„É´ÈÄ≤Âá∫„Å´ÁéãÊâã„ÄÄ‰∏ñÁïå5‰Ωç„Éª„Ç´„É´„Éá...   \n",
              "3        verde2730  RT @SportsHochi: #T„É™„Éº„Ç∞ „ÉªÊó•Êú¨ÁîüÂëΩ„ÄÅÊ£Æ„Åï„Åè„Çâ„ÅåÊÆäÂã≤„ÅÆÂãùÂà©„ÄÄ #Âπ≥ÈáéÁæéÂÆá...   \n",
              "4   hoodgrindvideo  ...... - https://t.co/ymCbaDGDoQ\\n#hoodgrind #...   \n",
              "\n",
              "           posted_time  \n",
              "0  2023-02-12 21:31:08  \n",
              "1  2023-02-12 21:31:00  \n",
              "2  2023-02-12 21:30:20  \n",
              "3  2023-02-12 21:30:14  \n",
              "4  2023-02-12 21:30:04  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c0a0c73a-7a4f-4aba-8714-b68e02f5402b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>username</th>\n",
              "      <th>text</th>\n",
              "      <th>posted_time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MGuindonRC</td>\n",
              "      <td>Belleterre est le dernier endroit au Qu√©bec qu...</td>\n",
              "      <td>2023-02-12 21:31:08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>TesoroMoctezuma</td>\n",
              "      <td>üòÜüßãü•™#FelizDomingo!#EnPortada #Portada de\\n#Port...</td>\n",
              "      <td>2023-02-12 21:31:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>verde2730</td>\n",
              "      <td>RT @SportsHochi: #T„É™„Éº„Ç∞ „ÉªÊù±‰∫¨„Åå„Éï„Ç°„Ç§„Éä„É´ÈÄ≤Âá∫„Å´ÁéãÊâã„ÄÄ‰∏ñÁïå5‰Ωç„Éª„Ç´„É´„Éá...</td>\n",
              "      <td>2023-02-12 21:30:20</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>verde2730</td>\n",
              "      <td>RT @SportsHochi: #T„É™„Éº„Ç∞ „ÉªÊó•Êú¨ÁîüÂëΩ„ÄÅÊ£Æ„Åï„Åè„Çâ„ÅåÊÆäÂã≤„ÅÆÂãùÂà©„ÄÄ #Âπ≥ÈáéÁæéÂÆá...</td>\n",
              "      <td>2023-02-12 21:30:14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>hoodgrindvideo</td>\n",
              "      <td>...... - https://t.co/ymCbaDGDoQ\\n#hoodgrind #...</td>\n",
              "      <td>2023-02-12 21:30:04</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c0a0c73a-7a4f-4aba-8714-b68e02f5402b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c0a0c73a-7a4f-4aba-8714-b68e02f5402b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c0a0c73a-7a4f-4aba-8714-b68e02f5402b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# You code here (Please add comments in the code):\n",
        "\n",
        "# hashtag\n",
        "\n",
        "import tweepy\n",
        "import pandas as pd\n",
        "\n",
        "consumer_key = \"u7L1lnR7HN85dn1qnTFO1cegb\"\n",
        "consumer_secret = \"QN1JrEmit2To46ZcwWAT4aI5QGWZXWRDDUPnMCWV5M66SFc8wT\"\n",
        "access_key = \"1144377060036620294-BSEicX3zH7hIhksbNZV9mrWFwa07cO\"\n",
        "access_secret = \"gxWMOodDq1nQAjix9mHEOUSAtgE7XH5ctHInm0XRslJce\"\n",
        "\n",
        "def authenticate():\n",
        "    # Using OAuthHandler to authorize into twitter account by passing consumer_key, consumer_secret, access_key and access_secret\n",
        "    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
        "    auth.set_access_token(access_key, access_secret)\n",
        "    # getting tweepy API by sending performed authentication\n",
        "    api = tweepy.API(auth)\n",
        "    return api\n",
        "\n",
        "def fetch_tweets(api, num_of_tweets, hashtag):\n",
        "    \"\"\"\n",
        "      params1: api, is an twitter api(tweepy), that has been authenticated to get the data from twitter\n",
        "      params2: num_of_tweets, total amount of tweets or posts required\n",
        "      params3: hashtag, topic or name on which we want to search for tweet\n",
        "      return: list of N num of all_tweets containing each tweet with info.\n",
        "    \"\"\"\n",
        "    # all_tweets = []\n",
        "    \"\"\" \n",
        "    With the help of tweepy Cursor, using search_api and hashtag query as parameters, fetching all the tweets from twitter on particular\n",
        "    topic of hashtag. The number of tweets to be fetched is handled with the help of items(num_of_tweets)\n",
        "    \"\"\"\n",
        "    # for tweet in tweepy.Cursor(api.search, q='#' + hashtag).items(num_of_tweets):\n",
        "    #     all_tweets.append(tweet)\n",
        "    all_tweets = [tweet for tweet in tweepy.Cursor(api.search, q='#' + hashtag).items(num_of_tweets)]\n",
        "    return all_tweets\n",
        "\n",
        "def fetch_tweet_info(tweet):\n",
        "    \"\"\"\n",
        "      params1: tweet, containing each post of tweet info\n",
        "      return: list of user_name, tweet_text and tweet_posted_at\n",
        "    \"\"\"\n",
        "    # fetching each tweet info, such as user_name, tweet_text, posted_time\n",
        "    return [str(tweet.user.screen_name), str(tweet.text), str(tweet.created_at)]\n",
        "\n",
        "def get_Tweets():\n",
        "    \"\"\"\n",
        "      return: dataframe containing the list of all tweets info with username, tweet_text, posted_time\n",
        "    \"\"\"\n",
        "    # Authentication of twitter devloper account\n",
        "    tweet_api = authenticate()\n",
        "    num_of_tweets = int(input(\"Enter number of tweets want to fetch: \"))\n",
        "    hashtag = input(\"Enter the hashtag: \")\n",
        "    all_tweets = fetch_tweets(tweet_api, num_of_tweets, hashtag)\n",
        "    # fetching each tweet info\n",
        "    tweet_info = list(map(fetch_tweet_info, all_tweets))\n",
        "    # setting all tweets info into dataframe with columns of usermname, text and posted_time, to analyze our data more effectively using numpy and pandas\n",
        "    df = pd.DataFrame(tweet_info, columns=[\"username\", \"text\", \"posted_time\"])\n",
        "    print(f\"dimensions of tweets: {df.shape}\")\n",
        "    return df\n",
        "\n",
        "df = get_Tweets()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0VeZv6HIiD5Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}